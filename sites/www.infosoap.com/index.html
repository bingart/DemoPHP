<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Info Soap's Home Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="css/style.css" type="text/css" rel="stylesheet" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-104181012-1', 'auto');
  ga('send', 'pageview');
</script>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-1799667010059789",
    enable_page_level_ads: true
  });
</script>
</head>
<body>
<div id="page_wrapper">
  <div id="header_wrapper">
    <div id="header">
      <h1>Info<font color="#FFDF8C">Soap</font></h1>
      <h2>Repository of Web Pages</h2>
    </div>
    <div id="navcontainer">
      <ul id="navlist">
        <li id="active"><a href="#" id="current">Home</a></li>
        <li><a href="http://www.infosoap.com/page0.html">Trend</a></li>
        <li><a href="#">News</a></li>
        <li><a href="#">Social</a></li>
        <li><a href="#">Business</a></li>
      </ul>
    </div>
  </div>
  <div id="left_side">
    <h3>Left Side</h3>
    <p>
		A number of important applications require local access to substantial portions of the web. Examples include traditional text search engines [9] [2], related page services [9] [1], and topic-based search and categorization services [18]. Such applications typically access, mine or index a local cache or repository of web pages, since performing their analyses directly on the web would be too slow. For example, the Google search engine [9] computes the PageRank [3] of every web page by recursively analyzing the web's link structure. The repository receives web pages from a crawler, which is the component responsible for mechanically finding new or modified pages on the web. At the same time, the repository offers applications an access interface (API) so that they may efficiently access large numbers of up-to-date web pages.
	</p>
    <h3>Left Side</h3>
    <div class='featurebox_side'></div>
    <p>
	</p>
  </div>
  <div id="right_side">
    <h3>Right Side</h3>
    <p>
		A web repository stores and manages a large collection of data "objects," in this case web pages. It is conceptually not that different from other systems that store data objects, such as file systems, database management systems, or information retrieval systems. However, a web repository does not need to provide a lot of the functionality that the other systems provide, such as transactions, or a general directory naming structure. Thus, the web repository can be optimized to provide just the essential services, and to provide them in a scalable and very efficient way.
	</p>
  </div>
  <div id="content">
    <h3>Center Content Title</h3>
    <div
	</div>
    <p>
		From the nature of their services, one can infer that all web search engines either construct, or have access to, a web repository. However, these are proprietary and often specific to the search application. In this paper, we have attempted to discuss, in an application-independent manner, the functions and features that would be useful in a web repository, and have proposed an architecture that provides these functions efficiently.
	</p>
	
	<p>
		A number of web-based services have used web repositories as part of their system architecture. However, often the repositories have been constructed on a much smaller scale and for a restricted purpose. For example, the WebGUIDE system [7] allows users to explore changes to the World Wide Web and web structure by supporting recursive document comparison. It tracks changes to a user-specified set of web pages using the AT&T Difference Engine (AIDE) [6] and provides a graphical visualization tool on top of AIDE. The AIDE version repository retrieves and stores only pages that have explicitly been requested by users. As such, the size of the repository is typically much smaller than the sizes targeted by WebBase. Similarly, GlimpseHTTP (now called WebGlimpse) [13] provides text-indexing and "neighborhood-based" search facilities on existing repositories of web pages. Here again, the emphasis is more on the actual indexing facility and much less on the construction and maintenance of the repository.
	</p>
	
	<p>
		The Internet Archive project aims to build a digital library for long-term preservation of web-published information. The focus of that project is on addressing issues relevant to archiving and preservation. Their target client population consists of scientists, sociologists, journalists, historians, and others who might want to use this information in the future for research purposes. On the other hand, our focus with WebBase has been on architecting a web repository in such a way that it can be kept relatively fresh, and be able to act as an immediate and current source of web information for a large number of existing applications.
	</p>

	
  </div>
  <div id="footer"> <a href="http://www.infosoap.com/trend.html">Trend</a> | <a href="#">News</a> | <a href="#">Social</a> | <a href="#">Business</a> <br />
    All Right Reserved by <a href="http://www.infosoap.com">InfoSoap.com</a> </div>
</div>
</body>
</html>
